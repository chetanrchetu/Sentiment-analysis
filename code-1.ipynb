{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- to unzip a zip file we use the `tarfile` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "# with tarfile.open(r'D:\\Python machine learning\\datasets\\movie_review\\aclImdb_v1.tar.gz') as tar:\n",
    "#     tar.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now we will load the data into a right formate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyprind \n",
    "import pandas as pd \n",
    "\n",
    "import os \n",
    "\n",
    "\n",
    "# #the change the `basepath` to the directory of the unzippped movie dataset\n",
    "# basepath=r'D:\\Python machine learning\\chapter-8,Sentiment analysis using ml\\aclImdb'\n",
    "\n",
    "# labels={'pos':1,'neg':0}\n",
    "\n",
    "# pbar=pyprind.ProgBar(50000)\n",
    "# df=pd.DataFrame()\n",
    "\n",
    "# for s in ('test','train'):\n",
    "#     for l in ('pos','neg'):\n",
    "        \n",
    "#         path=os.path.join(basepath,s,l)\n",
    "\n",
    "#         for file in sorted(os.listdir(path)):\n",
    "#             with open(os.path.join(path,file),'r',encoding='utf-8') as infile:\n",
    "#                 txt=infile.read()\n",
    "\n",
    "#             df=df.append([[txt,labels[l]]],ignore_index=True)\n",
    "#             pbar.update()\n",
    "\n",
    "# df.columns=['review','sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- converting the assembled data into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "# np.random.seed(0)\n",
    "\n",
    "# df=df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "# df.to_csv(r'D:\\Python machine learning\\datasets\\movie_review\\movie_data.csv',encoding='utf-8',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(r'D:\\Python machine learning\\datasets\\movie_review\\movie_data.csv',encoding='utf-8')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now we will convert the text data into bag of word using the `CountVectorizer` like converting the text data into numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "#example code of workiing\n",
    "count=CountVectorizer()\n",
    "\n",
    "docs=np.array([\n",
    "    'The sun is shining',\n",
    "    'The weather is sweet',\n",
    "    'The sun is shining the weather is sweet,'\n",
    "    'and one and one is two',\n",
    "\n",
    "])\n",
    "\n",
    "bag=count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1, 1, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 1, 1, 0, 1],\n",
       "       [2, 3, 2, 1, 1, 1, 2, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 6, 'sun': 4, 'is': 1, 'shining': 3, 'weather': 8, 'sweet': 5, 'and': 0, 'one': 2, 'two': 7}\n"
     ]
    }
   ],
   "source": [
    "print(count.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- here we will use the `TfidfTransformer` that is used to perform \n",
    "`tfidf(term frequency inverse document frequency)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.43 0.   0.56 0.56 0.   0.43 0.   0.  ]\n",
      " [0.   0.43 0.   0.   0.   0.56 0.43 0.   0.56]\n",
      " [0.5  0.45 0.5  0.19 0.19 0.19 0.3  0.25 0.19]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "tfidf=TfidfTransformer(\n",
    "    use_idf=True,\n",
    "    norm='l2',\n",
    "    smooth_idf=True\n",
    ")\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "print(tfidf.fit_transform(count.transform(docs)).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now we will learn to `clean the text data` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is seven.<br /><br />Title (Brazil): Not Available'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[0,'review'][-50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now we will use the `regular expression` for parsing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def preprocessing(text):\n",
    "    text=re.sub('<[^>]*>','',text)\n",
    "    emoticons=re.findall('(?::|,|=)(?:-)?(?:\\)|\\(|D|P)',text)\n",
    "\n",
    "    text=(re.sub('[\\W]+',' ',text.lower()))+' '.join(emoticons).replace('-','')\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test :) :( :)'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing(df.loc[0,'review'][-50:])\n",
    "preprocessing('</a>This :) is :( a test :-)!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].apply(preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now we will split the text at the whitespace using the .split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runners', 'like', 'running', 'and', 'thus', 'they', 'run']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split(' ')\n",
    "\n",
    "\n",
    "tokenizer('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now we will apply the stemming technique for tokeniztion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer \n",
    "porter=PorterStemmer()\n",
    "\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word=w) for w in text.split()]\n",
    "\n",
    "tokenizer_porter('runners like running and thus they run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now we will use the stopwords function to remove stop-words as it contains non useful words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['runner', 'like', 'run', 'run', 'lot']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "stop=stopwords.words('english')\n",
    "\n",
    "[word for word in tokenizer_porter('a runner likes running and runs a lot')[-10:] if word not in stop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now we will split the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=df.loc[:25000,'review'].values\n",
    "y_train=df.loc[:25000,'review'].values\n",
    "X_test=df.loc[25000:,'review'].values\n",
    "y_test=df.loc[25000:,'review'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now we will use the GridSearchCV to find the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV \n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.linear_model import LogisticRegression \n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "# tfidf=TfidfVectorizer(strip_accents=None,\n",
    "#                       lowercase=False,\n",
    "#                       preprocessor=None)\n",
    "\n",
    "# param_grid=[\n",
    "#     {\n",
    "#         'vect__ngram_range':[(1,1)],\n",
    "#         'vect__stop_words':[stop,None],\n",
    "#         'vect__tokenizer':[tokenizer,tokenizer_porter],\n",
    "#         'clf__penalty':['l1','l2'],\n",
    "#         'clf__C':[1.0,10.0,100.0],\n",
    "\n",
    "#     },\n",
    "#     {\n",
    "#         'vect__ngram_range':[(1,1)],\n",
    "#         'vect__stop_words':[stop,None],\n",
    "#         'vect__tokenizer':[tokenizer,tokenizer_porter],\n",
    "#         'vect__use_idf':[False],\n",
    "#         'vect__norm':[None],\n",
    "#         'clf__penalty':['l1','l2'],\n",
    "#         'clf__C':[1.0,10.0,100.0],\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# #final model\n",
    "# lr_tfidf=Pipeline(\n",
    "#     [\n",
    "#         ['vect',tfidf],\n",
    "#         ['clf',LogisticRegression(random_state=0,solver='liblinear')]\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# #grid search model\n",
    "# gs_lr_tfidf=GridSearchCV(estimator=lr_tfidf,param_grid=param_grid,cv=3,scoring='accuracy',verbose=2,n_jobs=2)\n",
    "\n",
    "# gs_lr_tfidf.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('best parameter set :%s'%(gs_lr_tfidf.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now we will perform out of core learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import re \n",
    "from nltk.corpus import stopwords\n",
    "stop=stopwords.words('english')\n",
    "\n",
    "def tokenizer(text):\n",
    "    text=re.sub('<[^>]*>','',text)\n",
    "    emoticons=re.findall('(?::|,|=)(?:-)?(?:\\)|\\(|D|P)',text)\n",
    "    text=re.sub('[\\W]+',' ',text.lower())+''.join(emoticons).replace('-','')\n",
    "\n",
    "    tokenized=[w for w in text.split() if w not in stop]\n",
    "\n",
    "    return tokenized "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now we will define a generator function, that reads and returns one document at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_docs(path):\n",
    "    with open(path,'r',encoding='utf-8') as csv:\n",
    "        next(csv) #skip the header\n",
    "\n",
    "        for line in csv:\n",
    "            text,label=line[:-3],int(line[-2])\n",
    "            yield text,label #this is will not return the entier csv but only one line at\n",
    "            # a time and when the next function is called it keeps returning one line until \n",
    "            #there is no more lines\n",
    "            #this helps in saving the memory while loading  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1974',\n",
       " 'teenager',\n",
       " 'martha',\n",
       " 'moxley',\n",
       " 'maggie',\n",
       " 'grace',\n",
       " 'moves',\n",
       " 'high',\n",
       " 'class',\n",
       " 'area',\n",
       " 'belle',\n",
       " 'greenwich',\n",
       " 'connecticut',\n",
       " 'mischief',\n",
       " 'night',\n",
       " 'eve',\n",
       " 'halloween',\n",
       " 'murdered',\n",
       " 'backyard',\n",
       " 'house',\n",
       " 'murder',\n",
       " 'remained',\n",
       " 'unsolved',\n",
       " 'twenty',\n",
       " 'two',\n",
       " 'years',\n",
       " 'later',\n",
       " 'writer',\n",
       " 'mark',\n",
       " 'fuhrman',\n",
       " 'christopher',\n",
       " 'meloni',\n",
       " 'former',\n",
       " 'la',\n",
       " 'detective',\n",
       " 'fallen',\n",
       " 'disgrace',\n",
       " 'perjury',\n",
       " 'j',\n",
       " 'simpson',\n",
       " 'trial',\n",
       " 'moved',\n",
       " 'idaho',\n",
       " 'decides',\n",
       " 'investigate',\n",
       " 'case',\n",
       " 'partner',\n",
       " 'stephen',\n",
       " 'weeks',\n",
       " 'andrew',\n",
       " 'mitchell',\n",
       " 'purpose',\n",
       " 'writing',\n",
       " 'book',\n",
       " 'locals',\n",
       " 'squirm',\n",
       " 'welcome',\n",
       " 'support',\n",
       " 'retired',\n",
       " 'detective',\n",
       " 'steve',\n",
       " 'carroll',\n",
       " 'robert',\n",
       " 'forster',\n",
       " 'charge',\n",
       " 'investigation',\n",
       " '70',\n",
       " 'discover',\n",
       " 'criminal',\n",
       " 'net',\n",
       " 'power',\n",
       " 'money',\n",
       " 'cover',\n",
       " 'murder',\n",
       " 'murder',\n",
       " 'greenwich',\n",
       " 'good',\n",
       " 'tv',\n",
       " 'movie',\n",
       " 'true',\n",
       " 'story',\n",
       " 'murder',\n",
       " 'fifteen',\n",
       " 'years',\n",
       " 'old',\n",
       " 'girl',\n",
       " 'committed',\n",
       " 'wealthy',\n",
       " 'teenager',\n",
       " 'whose',\n",
       " 'mother',\n",
       " 'kennedy',\n",
       " 'powerful',\n",
       " 'rich',\n",
       " 'family',\n",
       " 'used',\n",
       " 'influence',\n",
       " 'cover',\n",
       " 'murder',\n",
       " 'twenty',\n",
       " 'years',\n",
       " 'however',\n",
       " 'snoopy',\n",
       " 'detective',\n",
       " 'convicted',\n",
       " 'perjurer',\n",
       " 'disgrace',\n",
       " 'able',\n",
       " 'disclose',\n",
       " 'hideous',\n",
       " 'crime',\n",
       " 'committed',\n",
       " 'screenplay',\n",
       " 'shows',\n",
       " 'investigation',\n",
       " 'mark',\n",
       " 'last',\n",
       " 'days',\n",
       " 'martha',\n",
       " 'parallel',\n",
       " 'lack',\n",
       " 'emotion',\n",
       " 'dramatization',\n",
       " 'vote',\n",
       " 'seven',\n",
       " 'title',\n",
       " 'brazil',\n",
       " 'available']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(next(stream_docs(path=r'D:\\Python machine learning\\datasets\\movie_review\\movie_data.csv'))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now we will create the get_minibatch function to get the docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(doc_streams,size):\n",
    "    docs,y=[],[]\n",
    "\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text,label=next(doc_streams)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    \n",
    "    except StopIteration:\n",
    "        return None,None \n",
    "\n",
    "    return docs,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer \n",
    "from sklearn.linear_model import SGDClassifier \n",
    "\n",
    "vect=HashingVectorizer(\n",
    "    decode_error='ignore',\n",
    "    n_features=2**21,\n",
    "    preprocessor=None,\n",
    "    tokenizer=tokenizer \n",
    ")\n",
    "\n",
    "clf=SGDClassifier(loss='log_loss',random_state=1)  #by setting the loss to log we are modifying the Stochastic Gradient Descent Classifier into LogisticRegression\n",
    "doc_stream=stream_docs(path=r'D:\\Python machine learning\\datasets\\movie_review\\movie_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now we start the out of core learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:23\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "pbar = pyprind.ProgBar(45)\n",
    "classes = np.array([0, 1])\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- assigning the the documents to test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.868\n"
     ]
    }
   ],
   "source": [
    "X_test,y_test=get_minibatch(doc_stream,size=5000)\n",
    "X_test=vect.transform(X_test)\n",
    "print('Accuracy %0.3f'%(clf.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now we will update the classifier with the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=clf.partial_fit(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now we will decompose the movie review into 10 categories using the subcategory of unsupervised `LatentDirichletAllocation` `LDA` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df=pd.read_csv(r'D:\\Python machine learning\\datasets\\movie_review\\movie_data.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count=CountVectorizer(stop_words='english',\n",
    "                      max_df=.1,\n",
    "                      max_features=5000)\n",
    "\n",
    "X=count.fit_transform(df['review'].values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- fitting the `LatentDirichletAllocation`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation \n",
    "\n",
    "lda=LatentDirichletAllocation(n_components=10,\n",
    "                              random_state=123,\n",
    "                              learning_method='batch')\n",
    "\n",
    "X_topics=lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5000)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1\n",
      "worst, minutes, awful, script, stupid, terrible, money\n",
      "Topic 2\n",
      "family, mother, father, children, girl, loved, kids\n",
      "Topic 3\n",
      "american, war, dvd, music, tv, history, german\n",
      "Topic 4\n",
      "human, audience, cinema, art, sense, feel, viewer\n",
      "Topic 5\n",
      "police, guy, car, dead, murder, wife, goes\n",
      "Topic 6\n",
      "horror, house, sex, girl, woman, blood, gore\n",
      "Topic 7\n",
      "role, performance, comedy, actor, performances, plays, played\n",
      "Topic 8\n",
      "series, episode, war, episodes, tv, season, action\n",
      "Topic 9\n",
      "book, version, original, read, novel, disney, effects\n",
      "Topic 10\n",
      "action, fight, guy, guys, cool, fun, minutes\n"
     ]
    }
   ],
   "source": [
    "n_tops_words=7 \n",
    "feature_name=count.get_feature_names_out()\n",
    "\n",
    "for topic_idx,topic in enumerate(lda.components_):\n",
    "    print('Topic %d'%(topic_idx+1))\n",
    "    print(', '.join([feature_name[i]\n",
    "                    for i in topic.argsort()[:-n_tops_words-1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Horror movie #1:\n",
      "I went into this with my hopes up, by twenty minutes into the movie I couldn't have been more let down. Despite thinking that this would be another horribly bad remake, I kept my hopes high that maybe...just maybe someone would get it right this time around. Sadly, Prom Night is about on the same qu ...\n",
      "\n",
      "Horror movie #2:\n",
      "It used to be that video distributors like Sub Rosa and Brain Damage Films would release low-budget, shot-on-video horror films to a select market of gorehounds that ate them up with glee. That's acceptable to me, because you could see these movies from a mile away with their shoddy box art and chee ...\n",
      "\n",
      "Horror movie #3:\n",
      "It seems like anybody can make a movie nowadays. It's like all you need is a camera, a group of people to be your cast and crew, a script, and a little money and walla you have a movie. Problem is that talent isn't always part of this equation and often times these kind of low budget films turn out  ...\n"
     ]
    }
   ],
   "source": [
    "horror=X_topics[:,0].argsort()[::-1]\n",
    "\n",
    "for iter_idx,movie_idx in enumerate(horror[:3]):\n",
    "    print('\\nHorror movie #%d:'%(iter_idx+1))\n",
    "    print(df['review'][movie_idx][:300],'...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A strong pilot, this two-hour episode does an excellent job of setting up the characters and background for \"Enterprise,\" the \"prequel\" to the original \"Star Trek\" series. It stumbles a few times into \"Trek\" convention and clich√©--candy-colored space strippers never seem to go out of style, and I can already foresee snickering references to T\\'pol as \"Seven of Vulcan\"--but the ensemble looks strong, the characters are well-drawn, and one can already see hints that this particular crew will have to be more resourceful, in different ways, than those of earlier (later?) series. Scott Bakula hits the right note as a captain with Kirk\\'s brashness and daring but without his smugness and swagger, and I look forward to the ways in which the series will feature the engineer, weapons master and communications officer (not just a glorified phone operator anymore!) as supporting players. The writers seem to have picked up on the one big mistake made in \"Star Trek: The Next Generation,\" \"Deep Space 9\" and \"Voyager\": Instead of starting with a big ensemble cast and giving characters short shrift, it\\'s starting with a smaller core of characters to which a little more variety can be added later--which I hope happens, because after about a half-dozen episodes, more variety will be needed.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][X_topics[:,7].argsort()[::-1][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_topics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([2198, 2208, 3958, ..., 3835, 1847, 1130], dtype=int64)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[lda.components_[5].argsort()[::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(count.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worst\n",
      "family\n",
      "american\n",
      "human\n",
      "police\n",
      "horror\n",
      "role\n",
      "series\n",
      "book\n",
      "action\n"
     ]
    }
   ],
   "source": [
    "for i in lda.components_:\n",
    "    print(feature_name[i.argsort()[::-1][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16597    I went into this with my hopes up, by twenty m...\n",
       "41862    It used to be that video distributors like Sub...\n",
       "4498     It seems like anybody can make a movie nowaday...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][X_topics[::,0].argsort()[::-1][:3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter -9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import os \n",
    "dest=os.path.join(r'D:\\Python machine learning\\chapter-9,Machine learning to web\\movieclassifier','pkl_objects')\n",
    "\n",
    "if not os.path.exists(dest):\n",
    "    os.makedirs(dest)\n",
    "\n",
    "pickle.dump(stop,open(os.path.join(dest,'stopwords.pkl'),'wb'),protocol=4) #wb:- write binary\n",
    "\n",
    "pickle.dump(clf,open(os.path.join(dest,'classifier.pkl'),'wb'),protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
